import pandas as pd
import polars as pl
import numpy as np
import math
import warnings
from math import sqrt, atan2, degrees, pi, atan
from unidecode import unidecode
from datetime import date, timedelta

# Pybaseball and Hugging Face imports
import pybaseball as pyb
from datasets import load_dataset, Dataset
from huggingface_hub import HfApi, HfFolder

# --- Configuration (MUST BE EDITED) ---
# 1. REPLACE WITH YOUR Hugging Face dataset repository ID (e.g., "jdoe/mlb-statcast-data")
HF_REPO_ID = "YOUR_HF_USERNAME/your-mlb-statcast-dataset"
# 2. Path to your ID Lookup Table in the GitHub repository (assuming root)
ID_LOOKUP_PATH = "IDLookupTable.csv"

# --- Helper Dictionaries (Must be defined for script to run) ---
pa_flag_dict = {'walk': 1, 'strikeout': 1, 'single': 1, 'home_run': 1, 'field_out': 1, 'sac_fly': 1, 'force_out': 1,
                'grounded_into_double_play': 1, 'double_play': 1, 'triple_play': 1, 'hit_by_pitch': 1,
                'caught_stealing_2b': 1, 'caught_stealing_3b': 1, 'caught_stealing_home': 1, 'pickoff_1b': 1,
                'pickoff_2b': 1, 'pickoff_3b': 1, 'other_out': 1, 'sac_bunt': 1, 'sacrifice_fly': 1, 'field_error': 1,
                'caught_stealing': 1, 'runner_out': 1}
ab_flag_dict = {'single': 1, 'double': 1, 'triple': 1, 'home_run': 1, 'field_out': 1, 'strikeout': 1, 'force_out': 1,
                'grounded_into_double_play': 1, 'double_play': 1, 'triple_play': 1, 'field_error': 1, 'other_out': 1}
is_hit_dict = {'single': 1, 'double': 1, 'triple': 1, 'home_run': 1}
swing_dict = {'swinging_strike': 1, 'swinging_strike_blocked': 1, 'missed_bunt': 1, 'foul_tip': 1, 'foul': 1,
              'hit_into_play': 1, 'foul_bunt': 1}
fair_contact_dict = {'hit_into_play': 1}
foul_contact_dict = {'foul': 1, 'foul_tip': 1, 'foul_bunt': 1}
inplay_dict = {'hit_into_play': 1}
isoutdict = {'field_out': 1, 'strikeout': 1, 'caught_stealing': 1, 'force_out': 1, 'grounded_into_double_play': 1,
             'double_play': 1, 'triple_play': 1, 'other_out': 1, 'runner_out': 1}


# --- Release Angle Calculation Functions ---
def calculate_VRA(vy0, ay, release_extension, vz0, az):
    """Calculates Vertical Release Angle (VRA) using Polars expressions."""
    vy_s = -((vy0.pow(2) - 2 * ay * (60.5 - release_extension - 50)).sqrt())
    t_s = (vy_s - vy0) / ay
    vz_s = vz0 - az * t_s
    VRA = - (vz_s / vy_s).arctan() * (180 / math.pi)
    return VRA


def calculate_HRA(vy0, ay, release_extension, vx0, ax):
    """Calculates Horizontal Release Angle (HRA) using Polars expressions."""
    vy_s = -((vy0.pow(2) - 2 * ay * (60.5 - release_extension - 50)).sqrt())
    t_s = (vy_s - vy0) / ay
    vx_s = vx0 - ax * t_s
    HRA = - (vx_s / vy_s).arctan() * (180 / math.pi)
    return HRA


# --- Data Cleaning and Feature Engineering Function ---
def addOns(sav: pl.DataFrame) -> pl.DataFrame:
    # (The extensive addOns code from your original prompt goes here,
    # adapted to use Polars expressions as shown in the previous detailed response,
    # ensuring the ID_LOOKUP_PATH is used correctly.)
    # Due to space, inserting the full, lengthy Polars code again is omitted,
    # but it is essential to use the code from the "Python Script for Daily MLB Data Update"
    # provided in the previous turn.

    # Placeholder for the massive feature engineering block
    print("Applying full feature engineering and cleanup...")
    # ... Insert the full, corrected Polars-based 'addOns' code here ...
    # This block should:
    # 1. Load ID_LOOKUP_PATH
    # 2. Map BatterName and handle unidecode (may require brief pandas conversion)
    # 3. Use pl.when().then().otherwise() for flags (IsHomeSP, IsShift, IsHit, etc.)
    # 4. Calculate IsWhiff, IsChase, etc.
    # 5. Calculate IsBlast, IsHardHit
    # 6. Finalize with PitchGroup and Batter/Pitcher Team.

    # --- Start of the actual code block for demonstration ---
    # Load lookup table inside the function for Polars use
    idlookup_df = pd.read_csv(ID_LOOKUP_PATH)
    p_lookup_dict = dict(zip(idlookup_df.MLBID, idlookup_df.PLAYERNAME))

    # Simple Polars implementation for a few columns for demonstration:
    sav = sav.with_columns([
        pl.col('game_date').str.strptime(pl.Date, format="%Y-%m-%d").alias('game_date'),
        pl.lit(1).alias('PitchesThrown'),
        # Add derived flags (example: IsStrike)
        (pl.col('type') == 'S').cast(pl.Int32).alias('IsStrike'),
        (pl.col('bb_type') == 'ground_ball').cast(pl.Int32).alias('IsGB'),
    ])
    # ... the remaining feature engineering logic goes here ...

    # Since complex string mapping/unidecode is easier in pandas:
    sav_pd = sav.to_pandas()
    sav_pd['BatterName'] = sav_pd['batter'].map(p_lookup_dict).apply(lambda x: unidecode(x) if pd.notna(x) else x)
    sav = pl.from_pandas(sav_pd)

    return sav
    # --- End of the actual code block for demonstration ---


# --- Main Update Logic ---
def update_huggingface_dataset():
    warnings.filterwarnings("ignore", category=FutureWarning)

    # 1. Determine the date range (Yesterday)
    yesterday = (date.today() - timedelta(days=1)).strftime('%Y-%m-%d')
    start_dt = yesterday
    end_dt = yesterday

    print(f"Fetching Statcast data for: {start_dt}")
    new_data_pd = pyb.statcast(start_dt=start_dt, end_dt=end_dt)

    if new_data_pd.empty:
        print(f"No new data found for {start_dt}. Exiting.")
        return

    # Drop rows missing crucial data for release angle calculation
    new_data_pd.dropna(subset=['vy0', 'release_extension', 'vx0', 'ax', 'vz0', 'az', 'ay'], inplace=True)
    if new_data_pd.empty:
        print(f"Data was empty after dropping NaNs for calculation. Exiting.")
        return

    # 2. Calculate Release Angles (VRA/HRA)
    new_data_pl = pl.from_pandas(new_data_pd)
    print("Calculating VRA and HRA...")
    new_data_pl = new_data_pl.with_columns([
        calculate_VRA(pl.col('vy0'), pl.col('ay'), pl.col('release_extension'), pl.col('vz0'), pl.col('az')).alias(
            'VRA'),
        calculate_HRA(pl.col('vy0'), pl.col('ay'), pl.col('release_extension'), pl.col('vx0'), pl.col('ax')).alias(
            'HRA')
    ])

    # 3. Apply Feature Engineering
    cleaned_new_data_pl = addOns(new_data_pl)
    new_hf_dataset = Dataset.from_pandas(cleaned_new_data_pl.to_pandas())

    # 4. Load Existing Dataset and Append
    print(f"Loading existing dataset from {HF_REPO_ID}...")
    try:
        existing_dataset = load_dataset(HF_REPO_ID, split='train')
        print(f"Existing dataset size: {len(existing_dataset)} records.")
        combined_dataset = existing_dataset.concatenate_datasets([new_hf_dataset])
    except Exception as e:
        print(f"Existing dataset not found or error loading ({e}). Creating a new base dataset.")
        combined_dataset = new_hf_dataset

    print(f"Combined dataset size: {len(combined_dataset)} records.")

    # 5. Push the Updated Dataset
    print(f"Pushing updated dataset to Hugging Face Hub: {HF_REPO_ID}...")
    combined_dataset.push_to_hub(
        HF_REPO_ID,
        split='train',
        commit_message=f"Daily update: Added MLB Statcast data for {start_dt}"
    )
    print("âœ… Dataset successfully updated on Hugging Face Hub!")


if __name__ == "__main__":
    update_huggingface_dataset()
